## 瓜子二手车数据爬取

该爬虫基于Pyhthon scrapy框架爬取瓜子二手车成都地区的数据信息，具体思路步骤如下：

1.创建项目，搭配虚拟环境，安装twisted、scrapy等三方依赖库。

2.终端输入scrapy startproject guazi 创建项目，进入spider目录，终端输入scrapy genspider guazi www.guazi.com , 创建爬虫文件。

3.解析瓜子二手车网站地址：进入网页我要买车成都地区，链接为:<https://www.guazi.com/cd/buy/o1/#bread>，数据共有50页，通过改变o后面的数字可获取不同页面的数据信息。

4.爬虫文件car.py: 类中先重写 def start_requests(self):方法，获取总页数进行for循环得到每个页面的url，并                yield Request(href, callback=self.parse_car, dont_filter=True)。                                                                              回调的方法为.parse_car，对数据进行解析，先获取页面得数据结构框架，在for循环一次湖区每个汽车对应的数据信息，赋值到定义的item模型对象中，返回。

5.模型item.py
```
class GuaziItem(scrapy.Item):
    collection = table = 'second-hand cars' # 集合
    name = scrapy.Field()  # 名称
    price = scrapy.Field()  # 价格
    date = scrapy.Field()  # 日期
    trip = scrapy.Field()  # 里程
    service = scrapy.Field()  # 服务
    tags = scrapy.Field()  # 标签
    pic = scrapy.Field()  # 图片
```

6.配置文件settings.py:     
```
ROBOTSTXT_OBEY = False     # 不遵循robots.txt规则 

DOWNLOADER_MIDDLEWARES = {
	'guazi.middlewares.SeleniumMiddleware': 543,    # selenium中间件
    'guazi.middlewares.TestMiddleware': 544,          # IP代理中间件
}        #   DOWNLOADER中间件

ITEM_PIPELINES = {
   'guazi.pipelines.GuaziPipeline': 300,
}      

SELENIUM_TIMEOUT = 10   # 超时

MONGO_URI = '47.103.14.96' 
MONGO_DB = 'spider'  # MONGO信息设置
```

7. 中间件middlewares.py:   定义了 SeleniumMiddleware中间件,获取chrome对象，对网页进行请求，模拟人的操作，打开网页时，
滚动页面的八分之一高度并睡眠两秒，以避免频繁的请求而导致爬取异常， 
                                                                                                                                         定义了 SeleniumMiddleware中间件,获取chrome对象，对网页进行请求，模拟人的操作，打开网页时，滚动页面的八分之一高度并睡眠两秒，以避免频繁的请求而导致爬取异常，
8. 管道pipelines：将爬取的数据存入mongo 数据库



